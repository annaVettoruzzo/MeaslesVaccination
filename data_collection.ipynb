{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install searchtweets\n",
    "\n",
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials\n",
    "from tweet_parser.tweet import Tweet\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "premium_search_args = load_credentials(\"./.twitter_keys.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet, geo='False'):\n",
    "    \n",
    "    # the same tags in all the tweets\n",
    "    tweet_id = tweet[\"id_str\"] #OK for future computations\n",
    "    created_at = tweet[\"created_at\"] #OK\n",
    "    username = tweet[\"user\"][\"id_str\"] #user_id/user/name/user_screenName???\n",
    "    #acctdesc = tweet.user.description\n",
    "    location = tweet[\"user\"][\"location\"] #OK\n",
    "    coordinate = tweet[\"coordinates\"]\n",
    "    #following = tweet[\"user\"][\"friends_count\"] #OK to study the diffusion\n",
    "    #followers = tweet[\"user\"][\"followers_count\"] #OK to study the diffusion\n",
    "    user_mentions = []\n",
    "    for result in tweet[\"entities\"][\"user_mentions\"]: # get the mentioned users in the tweet and store them in a list\n",
    "        user_mentions.append(result[\"screen_name\"]) \n",
    "    #totaltweets = tweet.user.statuses_count\n",
    "    #usercreatedts = tweet.user.created_at\n",
    "    #tweetcreatedts = tweet.created_at\n",
    "    retweetcount = tweet[\"retweet_count\"] #OK to study the diffusion\n",
    "    hashtags = []\n",
    "    for result in tweet[\"entities\"][\"hashtags\"]: # get the hastags and store in a list\n",
    "        hashtags.append(result[\"text\"])\n",
    "    \n",
    "    full_text = tweet[\"text\"]  \n",
    "    \n",
    "    clean_tweet = [tweet_id, created_at, username, location, coordinate, user_mentions, retweetcount, full_text, hashtags] # save all the data in a list \n",
    "    \n",
    "    if(geo == 'True'):\n",
    "        #Extract only the tweets geolocalized\n",
    "        clean_tweet_geo = 0      \n",
    "        if(location == None and coordinate == None):\n",
    "            print('no geo')\n",
    "        else:\n",
    "            clean_tweet_geo = [tweet_id, created_at, username, location, coordinate, user_mentions, retweetcount, full_text, hashtags]\n",
    "        return clean_tweet, clean_tweet_geo\n",
    "        \n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Two empty dataframes: one for all the cleaned tweets and one for the geolocalized ones.\n",
    "'''\n",
    "# Create a dataframe to store all the tweets\n",
    "tweets_df = pd.DataFrame(columns = ['tweet_id', 'created_at', 'username', 'location', 'coordinates', 'user_mentions', 'retweetcount', 'full_text', 'hashtags'])\n",
    "\n",
    "# Create a dataframe to store only the tweets that are geolocalized\n",
    "tweets_df_geo =  pd.DataFrame(columns = ['tweet_id', 'created_at', 'username', 'location', 'coordinates', 'user_mentions', 'retweetcount', 'full_text', 'hashtags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Cycle from may to december. Query the twitter api two times per month (one time for the first 15 days and another time for the last 15).\n",
    "    Use 28 days per month to avoid any problem with February.\n",
    "'''\n",
    "for i in range(5,12,1): \n",
    "    \n",
    "    rule = gen_rule_payload(\"libertà di scelta vaccinale morbillo\", results_per_call=100, from_date=\"2017-\"+str(i)+\"-01\", to_date=\"2017-\"+str(i)+\"-15\")\n",
    "    rule2 = gen_rule_payload(\"libertà di scelta vaccinale morbillo\", results_per_call=100, from_date=\"2017-\"+str(i)+\"-16\", to_date=\"2017-\"+str(i)+\"-28\")\n",
    "    rs = ResultStream(rule_payload=rule, max_results=100, max_pages=1, **premium_search_args)\n",
    "    rs2 = ResultStream(rule_payload=rule2, max_results=100, max_pages=1, **premium_search_args)\n",
    "\n",
    "    tweets = list(rs.stream()) + list(rs2.stream())\n",
    "\n",
    "    \n",
    "    #create json file and store in the folder data/tweets\n",
    "    for tweet in tweets:\n",
    "        filename =  tweet.id + \".json\"\n",
    "        with open(f'data/tweets/' + filename, 'w') as f:\n",
    "            json.dump(tweet, f)\n",
    "            f.close()\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tweet_id = tweet[\"id\"]\n",
    "        id_ = tweet_id\n",
    "        path = f'data/tweets/{tweet_id}.json'\n",
    "        clean_tweets = [] #store all the clean tweets\n",
    "        clean_tweets_geo = [] #store the tweets that are geolocalized\n",
    "        \n",
    "        # get the jsons and clean the tweets\n",
    "        # append to \"clean_tweets\" and to \"clean_tweets_geo\" if they are geolocalized\n",
    "        # if the tweet is geolocalized save its json in data/tweets_geo\n",
    "        with open(path) as json_file:\n",
    "            tweet = json.load(json_file)\n",
    "            clean_tweet, clean_tweet_geo = clean(tweet, geo='True')\n",
    "            clean_tweets.append(clean_tweet) # use the method in order to get the filtered tweet and append it to a list\n",
    "            if(clean_tweet_geo != 0):\n",
    "                clean_tweets_geo.append(clean_tweet_geo)\n",
    "                # Insert the json in file in a new folder\n",
    "                filename_geo = str(id_) + \".json\"\n",
    "                with open(f'data/tweets_geo/' + filename_geo, 'w') as f:\n",
    "                    json.dump(tweet, f)\n",
    "                    f.close()\n",
    "    \n",
    "        # create dataframe from the filtered tweets using the list created reading all the json files\n",
    "        tweet_df = pd.DataFrame(clean_tweets)\n",
    "        tweet_df.columns = ['tweet_id', 'created_at', 'username', 'location', 'coordinates', 'user_mentions', 'retweetcount', 'full_text', 'hashtags']\n",
    "\n",
    "    \n",
    "        #add data to the already existing dataframe\n",
    "        tweets_df.loc[len(tweets_df)]=(clean_tweets[0])  \n",
    "        \n",
    "        if(len(clean_tweets_geo)!=0):\n",
    "            tweets_df_geo.loc[len(tweets_df_geo)]=(clean_tweets_geo[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dataframe in path\n",
    "path = \"data/allTweets.csv\"\n",
    "tweets_df.to_csv(path,header=True)\n",
    "\n",
    "path_geo = \"data/allTweets_geo.csv\"\n",
    "tweets_df_geo.to_csv(path_geo,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the italianTweets dataset\n",
    "df_italianTweets = pd.read_csv(\"dataset/morbillo_vaccinazioni/allTweets_geo.csv\")\n",
    "df_italianTweets.drop('Unnamed: 0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
